ccc <- cc[,-c("CementCol","FlyAshCol")]
ccc <- cc[-c("CementCol","FlyAshCol")]
ccdf <- data.fram(cc)
ccdf <- data.frame(cc)
ccc <- ccdf[-c("CementCol","FlyAshCol")]
ccc <- ccdf[1:9]
featurePlot(x=ccc,y=ccc$CompressiveStrength,plot="pairs")
plot(concrete$Age,pch=19,col=3)
plot(concrete$CompressiveStrength,pch=19,col=1)
plot(concrete$CompressiveStrength,concrete$Aage,pch=19,col=1)
plot(concrete$CompressiveStrength,concrete$Age,pch=19,col=1)
plot(concrete$CompressiveStrength,pch=19,col=1)
plot(concrete$CompressiveStrength,concrete$Age,pch=19,col=1)
plot(concrete$Age,pch=19,col=3)
plot(concrete$CompressiveStrength,pch=19,col=1)
cor(cc)
cor(ccc)
cor(ccc)
cor(ccc)
fit <- lm( CompressiveStrength ~ Cement + CoarseAggregate + Age)
fit <- lm( CompressiveStrength ~ Cement + CoarseAggregate + Age, data=ccc)
summary(fit)
summary(ccc)
histo(ccc$SuperPlasticizer)
hist(ccc$SuperPlasticizer)
head(ccc)
hist(ccc$Superplasticizer)
cccc <- ccc[,sp1 := ln(Superplasticizer+1)]
cccc <- ccc[,sp1 := log(Superplasticizer+1)]
class(ccc)
ccc <- data.table(ccc)
cccc <- ccc[,sp1 := log(Superplasticizer+1)]
hist[cccc$sp1]
cccc
hist[cccc$sp1]
sp1 <- cccc$sp1
hist(sp1)
hist(sp1,50)
hist(cccc$sp1)
hist(cccc$sp1,50)
hist(cccc$Superplasticizer,50)
hist(cccc$sp1,50)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(adData)
s1 <- 1.5
s2 <- 1.8
n1 <- 9
n2 <- 9
num <- ((s1*s2/n1) + (s2*s2)/n2)^2
fk1 <- s1*s1/n1
fk2 <- s2*s2/n2
num <- (f1 + f2)^2
num <- (fk1 + fk2)^2
denom <- (fk1)^2*(n1-1) + (fk2)^2*(n2-1)
num/denom
num
denom
t <- (-3-1)/sqrt(fk1 + fk2)
t
df <- num/denom
pt(t,df)
pt(t,8)
fk1
fk2
2.25/9
denom <- (fk1)^2/(n1-1) + (fk2)^2/(n2-1)
denom
num/denom
df <- nu/denom
df <- num/denom
pt(t,df)
-4 + c(1,-1)*sqrt(9)*qt(0.975,df)/sqrt(sqrt(num))
-4 + c(1,-1)*sqrt(9)*qt(0.975,df)/sqrt(num)
-4 + c(1,-1)*sqrt(9)*qt(0.975,df)/num
-4 + c(1,-1)*sqrt(9)*qt(0.975,df)/sqrt(denom)
-4 + c(1,-1)*sqrt(9)*qt(0.975,df)/denom
sqrt(num)
sqrt(sqrt(num))
-4 + c(1,-1)*qt(0.975,df)/sqrt(sqrt(num))
-4 + c(1,-1)*qt(0.975,df)/sqrt(num)
-4 + c(1,-1)*qt(0.975,df)/denom
-4 + c(1,-1)*sqrt(9)*qt(0.95,df)/sqrt(sqrt(num))
-4 + c(1,-1)*qt(0.975,df)/denom
-4 + c(1,-1)*qt(0.975,df)/sqrt(sqrt(num))
-4 + c(1,-1)*qt(0.95,df)/sqrt(sqrt(num))
-4 + c(-1,1)*qt(0.95,df)/sqrt(sqrt(num))
(c[1]+c[2])/2
ci <- -4 + c(-1,1)*qt(0.95,df)/sqrt(sqrt(num))
(ci[1]+ci[2])/2
ai[1] <- -5.044
a1 <- -5.044
a2 <- -2.956
(a1+a2)/2
ci <- -4 + c(-1,1)*qt(0.95,16)/sqrt(sqrt(num))
ci
df
sqrt(sqrt(num))
ss <- sqrt(sqrt(num))
0 / 22
9 / ss
2/1.5
3/1.5
3/1.8
ci <- -4 + c(-1,1)*qt(0.95,16)/1.8
ci <- -4 + c(-1,1)*qt(0.95,16)/1.9
ci <- -4 + c(-1,1)*qt(0.95,16)/1.7
3/(sqrt(1.5^2 + 1.8^2))
3/(sqrt(1.5^2/2 + 1.8^2/2))
bl <- c(140, 138, 150, 148, 135)
fu <- c(132, 135, 151, 146, 130)
t.test(fu, bl, alternative = "two.sided", paired = TRUE)
b <- fu-bl
t <- sqrt(5)*mean(b)/sd(b)
t
2*pt(t,4)
s <- sqrt(mean(bl)/5  + mean(fu)/5)
s
s <- sqrt(16/5  + 16/5)
s
t <- -3.4/s
t
2*pt(t,4)
4/sqrt(5)
s <- 4/sqrt(5)
s
2*pt(t,8)
s <- sqrt(16/5  + 16/5)
bl <- c(140, 138, 150, 148, 135)
fu <- c(132, 135, 151, 146, 130)
m <- mean(fu-bl)
s <- sd(fu-bl)
t <- sqrt(2)*m/s
t
s
m
pt(t,4)
t <- sqrt(4)*m/s
t
t <- sqrt(5)*m/s
t
pt(t,4)
2*pt(t,4)
m + s*qt(t,4)
qt(t,4)
m + s*qt(0.95,4)
m + s*qt(0.975,4)
m + c(-1,1)* s*qt(0.975,4)
m + c(-1,1)* s*qt(0.975,4)/sqrt(5)
sp <- sd(fu-bl)
s
sp
su <- sqrt(sd(bl)^2/sqrt(5) + sd(fu)^2/sqrt(5))
su
tu <- m/su
tu
2*pt(t,8)
su <- sqrt(sd(bl)^2/5 + sd(fu)^2/5
)
su <- sqrt(sd(bl)^2/5 + sd(fu)^2/5)
su
tu <- m/su
tu
2*pt(t,8)
2*pt(tu,8)
2*pt(tu,4)
2*pt(tu,8)
m + c(-1,1)*su*qt(0.975,4)
t.test(bl-fu)
t.test(fu-bl)
t.test(fu,bl)
340 / 28
m + c(-1,1)*su*qt(0.975,8)
library(data.table)
library(ElemStatLearn)
library(randomForest)
library(caret)
otrn <- data.frame(read.csv("pml-training.csv"))
otst <- data.frame(read.csv("pml-testing.csv"))
cnstst <- colnames(otrn)[7:59]
cnstrn <- c()
lntrn <- length(otrn)
lntst <- length(otst)
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
ntst <- data.table(user_name=otst[["user_name"]])
for (i in 1:length(cnstst))
{
cn <- cnstst[[i]]
if (cn=="num_window") next
clstst <- class(otst[[cn]])
clstrn <- class(otrn[[cn]])
if (clstst!="numeric" && clstst !="integer")
{
print(sprintf("%d tst %s is not numeric/integer but %s",i,cn,clstst))
next
}
if (clstrn!="numeric" && clstrn !="integer")
{
print(sprintf("%d trn %s is not numeric/integer but %s",i,cn,clstrn))
next
}
cnstrn <- c(cnstrn,cn)
ntrn[[cn]] <- otrn[[cn]]
ntst[[cn]] <- otst[[cn]]
}
ona <- sum(is.na(otrn))
nna <- sum(is.na(ntrn))
msg <- sprintf("Original training na count:%d  - After processing:%d",ona,nna)
print(msg)
set.seed(2718)
fit <- randomForest(classe ~ ., ntrn, importance=T)
ptrn <- predict(fit, ntrn)
confusionMatrix(ptrn, ntrn$classe)
varImpPlot(fit)
ptst <- predict(fit, ntst)
ptst
library(data.table)
library(ElemStatLearn)
library(randomForest)
library(caret)
otrn <- dataframe(read.csv("pml-training.csv"))
otst <- dataframe(read.csv("pml-testing.csv"))
cnstst <- colnames(otrn)[7:59]
cnstrn <- c()
lntrn <- length(otrn)
lntst <- length(otst)
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
ntst <- data.table(user_name=otst[["user_name"]])
for (i in 1:length(cnstst))
{
cn <- cnstst[[i]]
if (cn=="num_window") next
clstst <- class(otst[[cn]])
clstrn <- class(otrn[[cn]])
if (clstst!="numeric" && clstst !="integer")
{
print(sprintf("%d tst %s is not numeric/integer but %s",i,cn,clstst))
next
}
if (clstrn!="numeric" && clstrn !="integer")
{
print(sprintf("%d trn %s is not numeric/integer but %s",i,cn,clstrn))
next
}
cnstrn <- c(cnstrn,cn)
ntrn[[cn]] <- otrn[[cn]]
ntst[[cn]] <- otst[[cn]]
}
ona <- sum(is.na(otrn))
nna <- sum(is.na(ntrn))
msg <- sprintf("Original training na count:%d  - After processing:%d",ona,nna)
print(msg)
set.seed(2718)
fit <- randomForest(classe ~ ., ntrn, importance=T)
ptrn <- predict(fit, ntrn)
confusionMatrix(ptrn, ntrn$classe)
varImpPlot(fit)
ptst <- predict(fit, ntst)
ptst
library(data.table)
library(ElemStatLearn)
library(randomForest)
library(caret)
# Data Processing
## Load the data
otrn <- data.table(read.csv("pml-training.csv"))
otst <- data.table(read.csv("pml-testing.csv"))
## Get rid of all non-numeric columns not present in both datasets
cnstst <- colnames(otrn)[7:59]
lntrn <- length(otrn)
lntst <- length(otst)
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
ntst <- data.table(user_name=otst[["user_name"]])
for (i in 1:length(cnstst))
{
cn <- cnstst[[i]]
if (cn=="num_window") next
clstst <- class(otst[[cn]])
clstrn <- class(otrn[[cn]])
if (clstst!="numeric" && clstst !="integer")
{
print(sprintf("%d tst %s is not numeric/integer but %s",i,cn,clstst))
next
}
if (clstrn!="numeric" && clstrn !="integer")
{
print(sprintf("%d trn %s is not numeric/integer but %s",i,cn,clstrn))
next
}
ntrn[[cn]] <- otrn[[cn]]
ntst[[cn]] <- otst[[cn]]
}
# Check the quality
ona <- sum(is.na(otrn))
nna <- sum(is.na(ntrn))
msg <- sprintf("Original training na count:%d  - After processing:%d",ona,nna)
print(msg)
# Model Fitting
## Random Forests
set.seed(2718)
rffit <- randomForest(classe ~ ., ntrn, importance=T)
prftrn <- predict(rffit, ntrn)
confusionMatrix(prftrn, ntrn$classe)
varImpPlot(rffit)
prftst <- predict(rffit, ntst)
prftst
## Boost Trees
set.seed(2718)
btfit <- train(classe ~ ., method="gbm", data=ntrn, verbose=F)
pbttrn <- predict(btfit)
confusionMatrix(ptbtrn, ntrn$classe)
varImpPlot(btfit)
pbttst <- predict(btfit, ntst)
pbttst
library(data.table)
library(ElemStatLearn)
library(randomForest)
library(caret)
# Data Processing
## Load the data
otrn <- data.table(read.csv("pml-training.csv"))
otst <- data.table(read.csv("pml-testing.csv"))
## Get rid of all non-numeric columns not present in both datasets
cnstst <- colnames(otrn)[7:59]
lntrn <- length(otrn)
lntst <- length(otst)
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
ntst <- data.table(user_name=otst[["user_name"]])
for (i in 1:length(cnstst))
{
cn <- cnstst[[i]]
if (cn=="num_window") next
clstst <- class(otst[[cn]])
clstrn <- class(otrn[[cn]])
if (clstst!="numeric" && clstst !="integer")
{
print(sprintf("%d tst %s is not numeric/integer but %s",i,cn,clstst))
next
}
if (clstrn!="numeric" && clstrn !="integer")
{
print(sprintf("%d trn %s is not numeric/integer but %s",i,cn,clstrn))
next
}
ntrn[[cn]] <- otrn[[cn]]
ntst[[cn]] <- otst[[cn]]
}
# Check the quality
ona <- sum(is.na(otrn))
nna <- sum(is.na(ntrn))
msg <- sprintf("Original training na count:%d  - After processing:%d",ona,nna)
print(msg)
# Model Fitting
## Random Forests
set.seed(2718)
rffit <- randomForest(classe ~ ., ntrn, importance=T)
prftrn <- predict(rffit, ntrn)
confusionMatrix(prftrn, ntrn$classe)
varImpPlot(rffit)
prftst <- predict(rffit, ntst)
prftst
## Boost Trees
set.seed(2718)
btfit <- train(classe ~ ., method="gbm", data=ntrn, verbose=F)
pbttrn <- predict(btfit)
confusionMatrix(ptbtrn, ntrn$classe)
varImpPlot(btfit)
pbttst <- predict(btfit, ntst)
pbttst
setwd("C:/DataCert/ML/Proj1")
library(data.table)
library(ElemStatLearn)
library(randomForest)
library(caret)
# Data Processing
## Load the data
otrn <- data.table(read.csv("pml-training.csv"))
otst <- data.table(read.csv("pml-testing.csv"))
## Get rid of all non-numeric columns not present in both datasets
cnstst <- colnames(otrn)[7:59]
lntrn <- length(otrn)
lntst <- length(otst)
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
ntst <- data.table(user_name=otst[["user_name"]])
for (i in 1:length(cnstst))
{
cn <- cnstst[[i]]
if (cn=="num_window") next
clstst <- class(otst[[cn]])
clstrn <- class(otrn[[cn]])
if (clstst!="numeric" && clstst !="integer")
{
print(sprintf("%d tst %s is not numeric/integer but %s",i,cn,clstst))
next
}
if (clstrn!="numeric" && clstrn !="integer")
{
print(sprintf("%d trn %s is not numeric/integer but %s",i,cn,clstrn))
next
}
ntrn[[cn]] <- otrn[[cn]]
ntst[[cn]] <- otst[[cn]]
}
# Check the quality
ona <- sum(is.na(otrn))
nna <- sum(is.na(ntrn))
msg <- sprintf("Original training na count:%d  - After processing:%d",ona,nna)
print(msg)
# Model Fitting
## Random Forests
set.seed(2718)
rffit <- randomForest(classe ~ ., ntrn, importance=T)
prftrn <- predict(rffit, ntrn)
confusionMatrix(prftrn, ntrn$classe)
varImpPlot(rffit)
prftst <- predict(rffit, ntst)
prftst
## Boost Trees
set.seed(2718)
btfit <- train(classe ~ ., method="gbm", data=ntrn, verbose=F)
install.packages("gbm")
## Boost Trees
set.seed(2718)
btfit <- train(classe ~ ., method="gbm", data=ntrn, verbose=F)
pbttrn <- predict(btfit)
confusionMatrix(ptbtrn, ntrn$classe)
varImpPlot(btfit)
pbttst <- predict(btfit, ntst)
pbttst
confusionMatrix(pbtrn, ntrn$classe)
confusionMatrix(pbttrn, ntrn$classe)
levels(pbttrn)
levels(ntrn$classe)
ntrn$classe
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
confusionMatrix(pbttrn, ntrn$classe)
library(data.table)
library(ElemStatLearn)
library(randomForest)
library(caret)
# Data Processing
## Load the data
otrn <- data.table(read.csv("pml-training.csv"))
otst <- data.table(read.csv("pml-testing.csv"))
## Get rid of all non-numeric columns not present in both datasets
cnstst <- colnames(otrn)[7:59]
lntrn <- length(otrn)
lntst <- length(otst)
ntrn <- data.table(user_name=otrn[["user_name"]],classe=otrn[["classe"]])
ntst <- data.table(user_name=otst[["user_name"]])
for (i in 1:length(cnstst))
{
cn <- cnstst[[i]]
if (cn=="num_window") next
clstst <- class(otst[[cn]])
clstrn <- class(otrn[[cn]])
if (clstst!="numeric" && clstst !="integer")
{
print(sprintf("%d tst %s is not numeric/integer but %s",i,cn,clstst))
next
}
if (clstrn!="numeric" && clstrn !="integer")
{
print(sprintf("%d trn %s is not numeric/integer but %s",i,cn,clstrn))
next
}
ntrn[[cn]] <- otrn[[cn]]
ntst[[cn]] <- otst[[cn]]
}
# Check the quality
ona <- sum(is.na(otrn))
nna <- sum(is.na(ntrn))
msg <- sprintf("Original training na count:%d  - After processing:%d",ona,nna)
print(msg)
# Model Fitting
## Random Forests
set.seed(2718)
rffit <- randomForest(classe ~ ., ntrn, importance=T)
prftrn <- predict(rffit, ntrn)
confusionMatrix(prftrn, ntrn$classe)
varImpPlot(rffit)
prftst <- predict(rffit, ntst)
prftst
## Boost Trees
set.seed(2718)
btfit <- train(classe ~ ., method="gbm", data=ntrn, verbose=F)
pbttrn <- predict(btfit)
confusionMatrix(pbttrn, ntrn$classe)
pbttst <- predict(btfit, ntst)
pbttst
confusionMatrix(pbttrn, ntrn$.outcome)
View(ntrn)
View(ntrn)
knit2html("PmlProject.rmd")
setwd("C:/DataCert/ML/PmlProject")
knit2html("PmlProject.rmd")
knit2html("PmlProject.Rmd")
library(knitr)
knit2html("PmlProject.Rmd")
BrowseURL("PmlProject.html")
BrowseUrl("PmlProject.html")
?Bro
?Bro*
)
BrowseURL("PmlProject.html")
knit2html("PmlProject.Rmd")
